#!/usr/bin/env python
# coding: utf-8

import pandas as pd
import numpy as np
import pickle

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier



malData = pd.read_csv("/home/ubuntu/Desktop/P5-Hardwall/MalwareClassifier/MalwareData.csv", sep="|")

benign = malData[0:41323].drop(["legitimate"], axis=1)
mal = malData[41323::].drop(["legitimate"], axis=1)

print("The shape of the benign dataset is: %s samples, %s features" %(benign.shape[0], benign.shape[1]))
print("The shape of the mal dataset is: %s samples, %s features" %(mal.shape[0], mal.shape[1]))

print(malData.columns)
print(malData.head(5))


print(benign.take([1]))
print(mal.take([1]))

# Tree Classifier


data_in = malData.drop(['Name', 'md5', 'legitimate'], axis=1).values
labels = malData['legitimate'].values


extratrees = ExtraTreesClassifier().fit(data_in, labels)
select = SelectFromModel(extratrees,prefit=True)

# Transform the input data using the selected features
data_in_new = select.transform(data_in)

# Print the shapes of the original and transformed data
print(data_in.shape, data_in_new.shape)

# Save the selected features to a file
selected_features = malData.drop(['Name', 'md5', 'legitimate'], axis=1).columns[select.get_support()]
selected_features_path = 'selected_features.pkl'
print(f"Selected features: {selected_features.tolist()}")
with open(selected_features_path, 'wb') as file:
    pickle.dump(selected_features, file)
print(f"Selected features saved to {selected_features_path}")


# Get the number of features in the transformed data
features = data_in_new.shape[1]

# Calculate feature importances using ExtraTreesClassifier
importences = extratrees.feature_importances_

# Get the indices of the features sorted by importance in descending order
indices = np.argsort(importences)[::-1]

# Print the feature ranking
for f in range(features):
    print("%d"%(f+1), malData.columns[2+indices[f]], importences[indices[f]])

# Split the data into training and testing sets
benign_train, benign_test, mal_train, mal_test = train_test_split(data_in_new, labels, test_size=0.2)

# Initialize & train
classif = RandomForestClassifier(n_estimators=50)
classif.fit(benign_train, mal_train)


print("The score of the Random Forrest Classifier is: ", classif.score(benign_test, mal_test) * 100)

# Predict the labels for the test data
result = classif.predict(benign_test)

# Compute the confusion matrix to evaluate the accuracy of the classification
conf_mat = confusion_matrix(mal_test, result)
print(conf_mat.shape)


print("False positives: ", conf_mat[0][1]/sum(conf_mat[0])*100)
print("False negatives: ", conf_mat[1][0]/sum(conf_mat[1])*100)

# Save the Random Forest model to a file
with open('random_forest_model.pkl', 'wb') as file:
    pickle.dump(classif, file)



# Gradient Boosting Classifier:

# Initialize & train
grad_boost = GradientBoostingClassifier(n_estimators=50)
grad_boost.fit(benign_train, mal_train)


print("The score of the Gradient Boosting Classifer is: ", grad_boost.score(benign_test, mal_test)*100)







